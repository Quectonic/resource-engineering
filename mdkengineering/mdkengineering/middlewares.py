# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html

from scrapy import signals
from scrapy.http import HtmlResponse
from scrapy.exceptions import IgnoreRequest
import re
import random
from lxml.html.clean import Cleaner
from lxml import html, etree
import json
from datetime import datetime

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter

















class MdkengineeringSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.
    
    """# üç™ **Status: _Implemented_** 
        
    **Description**
    -----------
        This middleware is triggered after the response has been processed by \
        downloader middlewares and before the response is passed to the spider. 
            
    Key Tasks
    ---------
        Check if the response HTML contains meaningful content.
    """

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is called before the response is passed to the spider for further processing.
            It checks if the response HTML:
            
            1. Contains a <p> tag.
            2. Contains 'Error' in the title.
            3. Contains 'This page does not exist.' in the content.
            
            If any of the above conditions are met, the response is flagged with 'skip_item_extraction',
            prompting the crawlspider to skip extracting the content for further processing but only
            follow the links on the page, e.g.
            
            - `https://www.wikidoc.org/index.php?title=category:<*>`
            - `https://www.wikidoc.org/index.php/The_WikiDoc_Living_Textbook_<*>` 
            - `https://www.wikidoc.org/index.php/Category:<*>` 
            - `https://www.wikidoc.org/index.php/` 

        **Parameters**
        ----------
            `response`: `Response`
            `spider`: `Spider`

        **Returns**
        -------
        `None`
            a.k.a. Proceeding with the request.
        """
        
        if (not response.xpath('//p') or 
            'Error' in response.xpath('//head/title/text()').get() or 
            len(response.xpath('//*[@id="mw-content-text"]/div[1]/div/p[contains(., "This page does not exist.")]')) != 0
        ):
            spider.logger.info(f"MdkengineeringSpiderMiddleware: Skipping page {response.url} - No meaningful content found in this.")
            response.meta['skip_item_extraction'] = True
        
        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        
        """# üç™ **Status: _Default_** 
        
        **Description**
        -----------
            Called after the spider has processed the response. This method is used to \
            process the results returned by the spider. 
        
        **Parameters**
        ----------
            `response`: `Response` 
            `result`: `Iterable` [generated by the spider]
            `spider`: `Spider` [instance of the spider]

        **Yields**
        ------
        `Iterable`
            _description_
        """

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        
        """# üç™ **Status: _Default_** 
        
        **Description**
        -----------
            Called when the spider or process_spider_input() method raises an exception. \
            This method is used to handle exceptions raised by the spider.
            
        **Parameters**
        ----------
            `response`: `Response`
            `exception`: `Exception`
            `spider`: `Spider`
        """

        spider.logger.error(f"MdkengineeringSpiderMiddleware: Exception occured when extracting html components: {exception}")

        # Should return either None or an iterable of Request or item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        
        
        """# üç™ **Status: _Default_** 
        
        **Description**
        -----------
            Called with the start requests of the spider. This method works similarly to the \
            process_spider_output() method, except that it doesn‚Äôt have a response associated.

        **Parameters**
        ----------
            `start_requests`: `Iterable`
            `spider`: `Spider`
            
        **Yields**
        ------
        `request`
            _description_
        """

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            Called when the spider is opened. This method is used to perform any initializations \
            that the spider may need before the spider starts crawling.
        
        """
        
        spider.logger.info("MdkengineeringSpiderMiddleware: Spider opened: %s" % spider.name)











##################################################################













class RequestTracingDownloaderMiddleware:
    
    """# üç™ **Status: _Implemented_** 
    
    **Description**
    -----------
        This middleware is used to log the request and response objects made by the spider.
        
    Key Tasks
    ---------
        1. Log the request object made by the spider.
        2. Log the response object received by the spider.
    
    """
    
    def process_request(self, request, spider):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is called when the request is made by the spider. It logs the request object.

        **Parameters**
        ----------
            `request`: `Request`
            `spider`: `Spider`

        **Returns**
        -------
        `None`
            a.k.a. Proceeding with the request.
        """
        
        request_info = (
            f"Request URL: {request.url if request.url else 'None'}\n"
            f"Method: {request.method if request.method else 'None'}\n"
            f"Headers: {dict(request.headers) if request.method else 'None'}\n"
            f"Body: {request.body.decode('utf-8') if request.body else 'None'}\n"
            f"Cookies: {request.cookies if request.cookies else 'None'}\n"
            f"Meta: {request.meta if request.meta else 'None'}\n"
            f"Priority: {request.priority if request.priority else 'None'}\n"
            f"Callback: {request.callback if request.callback else 'None'}\n"
            f"Errback: {request.errback if request.errback else 'None'}\n"
            f"Dont Filter: {request.dont_filter if request.dont_filter else 'None'}\n"
            f"Flags: {request.flags if request.flags else 'None'}\n"
        )
        spider.logger.info(f"RequestTracingDownloaderMiddleware: Request Made:\n{request_info}")
        return None
        
    def process_response(self, request, response, spider):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is called when the response is received by the spider. It logs the response object.

        **Parameters**
        ----------
            `request`: `Request`
            `response`: `Response`
            `spider`: `Spider`

        **Returns**
        -------
        `response`
            response to be processed by the spider
        """
        
        response_info = (
            f"Response URL: {response.url if response.url else 'None'}\n"
            f"Status Code: {response.status if response.status else 'None'}\n"
            f"Headers: {dict(response.headers) if response.headers else 'None'}\n"
            f"Body: {response.text[:500] + '...' if len(response.text) > 500 else response.text}\n"
            f"Cookies: {response.request.cookies if response.request and response.request.cookies else 'None'}\n"
            f"Meta: {response.meta if hasattr(response, 'request') and response.request and response.meta else 'None'}\n"
            f"Encoding: {response.encoding if response.encoding else 'None'}\n"
            f"Flags: {response.flags if response.flags else 'None'}\n"
            f"Request URL: {response.request.url if response.request and response.request.url else 'None'}\n"
        )
        spider.logger.info(f"RequestTracingDownloaderMiddleware: Response Object:\n{response_info}")
        return response
























class RotateUserAgentDownloaderMiddleware:
    
    """# üç™ **Status: _Implemented_** 
    
    **Description**
    -----------
        This middleware is used to rotate the User-Agent header of the request object.
        
    Key Tasks
    ---------
        1. Rotate the User-Agent header of the request object.
    """
    
    def __init__(self, user_agents) -> None:
        self.user_agents = user_agents

    @classmethod
    def from_crawler(cls, crawler):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is used by Scrapy to create your spiders. It also extraacts the list of \
            'USER_AGENTS' from the settings of the spider project, then set it as an attribute of the \
            middleware instance.
            
        **Parameters**
        ----------
            `crawler`: `Crawler`

        **Returns**
        -------
        `middleware`
            instance of the middleware
        """
        # This method is used by Scrapy to create your spiders.
        
        # 'cls' here refers to the CustomDownloaderMiddleware class itself.
        user_agents = crawler.settings.get('USER_AGENTS', [])
        
        # Creates an instance of the CustomDownloaderMiddleware class.
        s = cls(user_agents)
        
        # Connects the spider_opened method to the spider_opened signal.
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        
        # **Returns** the instance of the middleware.
        return s

    
    def process_request(self, request, spider):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is called when the request is made by the spider. It rotates the User-Agent header \
            and modifies the request header field 'User-Agent' to a random choice from the list of user agents.

        **Parameters**
        ----------
            `request`: `Request`
            `spider`: `Spider`
        
        **Returns**
        -------
        `None`
            a.k.a. Proceeding with the request.
        """
        
        request.headers['User-Agent'] = random.choice(self.user_agents)
        spider.logger.info(f'RotateUserAgentDownloaderMiddleware: Using User-Agent: {request.headers["User-Agent"]}')
        return None
    
    
    def spider_opened(self, spider):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is called when the spider is opened. It logs a message indicating that the spider \
            has been opened.
            
        **Parameters**
        ----------
            `spider`: `Spider`
        
        """
        spider.logger.info(f"RotateUserAgentDownloaderMiddleware: Spider opened: %s" % spider.name)

























class WikiDocURLFilterDownloaderMiddleware:
    
    """# üç™ **Status: _Implemented_** 
    
    **Description**
    -----------
        This middleware is used to filter out WikiDoc URLs that are not wanted to be crawled.
        
    Key Tasks
    ---------
        1. Block certain WikiDoc URLs, refer to the 'DENY_URL_PATTERNS_WIKIDOC' in the settings.
    """
    
    def __init__(self, deny_patterns) -> None:
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method initializes the middleware instance with the deny patterns from the settings.
            
        **Parameters**
        ----------
            `deny_patterns`: `list`
                a list of regular expressions to match the URLs that are not wanted to be crawled.
        """
        
        self.deny_patterns = [re.compile(pattern) for pattern in (deny_patterns or [])]
        
    @classmethod
    def from_crawler(cls, crawler):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is used by Scrapy to create your spiders. It also extraacts the list of \
            'DENY_URL_PATTERNS_WIKIDOC' from the settings of the spider project, then set it as an attribute of the \
            middleware instance.

        **Parameters**
        ----------
            `crawler`: `Crawler`

        **Returns**
        -------
        `middleware`
            instance of the middleware
        """
        
        ## Those patterns would not be parsed nor used for link follow logics
        s = cls(deny_patterns=crawler.settings.get('DENY_URL_PATTERNS_WIKIDOC', []))
        
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s
    
    def spider_opened(self, spider):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is called when the spider is opened. It logs a message indicating that the spider \
            has been opened.
            
        **Parameters**
        ----------
            `spider`: `Spider`
            
        """
        
        self.spider = spider
        spider.logger.info(f"WikiDocURLFilterDownloaderMiddleware: Spider opened: %s" % spider.name)
        
    def process_request(self, request, spider):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is called when the request is made by the spider. It filters out the WikiDoc URLs \
            that are not wanted to be crawled by the spider.
            
            It checks the URL of the request 

        **Parameters**
        ----------
            `request`: `Request`
            `spider`: `Spider`

        **Returns**
        -------
        `None`
            a.k.a. Proceeding with the request.

        Raises
        ------
        `IgnoreRequest`
            request is skipped and never reaches the spider, therefore not followed
        """
        
        if re.search(r'index\.php.*', request.url) and 'wikidoc.org' in request.url:
            return None  # Allow the request to continue
        else:
            spider.logger.info(f"WikiDocURLFilterDownloaderMiddleware: Blocked non-wikiDoc URL or WikiDoc URL without index.php: {request.url}")
            raise IgnoreRequest  # Block the request by raising IgnoreRequest
        
    def process_response(self, request, response, spider):
        
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is called when the response is received by the spider. It filters out the WikiDoc URLs \
            that are not wanted to be crawled by the spider. It is processed before the response reaches the spider.

        **Parameters**
        ----------
            `request`: `Request`
            `response`: `Response`
            `spider`: `Spider`

        **Returns**
        -------
        `response`
            `response` to be processed by the spider
        
        """
        for pattern in self.deny_patterns:
            if pattern.search(request.url):
                spider.logger.info(f'WikiDocURLFilterDownloaderMiddleware: Blacklist URL Detected: Response Blocked from Entering Spider')
                spider.logger.info(f'WikiDocURLFilterDownloaderMiddleware: Blocked Pattern: {pattern}')
                raise IgnoreRequest(f"WikiDocURLFilterDownloaderMiddleware: Blocked URL: {request.url}")
        return response



























class MdkengineeringDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.
    
    """# üç™ **Status: _Implemented_**

    **Description**
    -----------
        This middleware is used to validate the response and \
        clean the HTML content of the response object and reconstruct the response object.
    """

    def __init__(self, deny_patterns) -> None:
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method initializes the middleware instance with the deny patterns from the settings.
            
        **Parameters**
        ----------
            `deny_patterns`: `list`
                a list of regular expressions to match the URLs that are not wanted to be crawled.
        """
        
        self.deny_patterns = [re.compile(pattern) for pattern in (deny_patterns or [])]
        
    @classmethod
    def from_crawler(cls, crawler):
        
        """# üç™ **Status: _Implemented_** 
        
        **Description**
        -----------
            This method is used by Scrapy to create your spiders. It also extraacts the list of \
            'DENY_URL_PATTERNS_WIKIDOC' from the settings of the spider project, then set it as an attribute of the \
            middleware instance.

        **Parameters**
        ----------
            `crawler`: `Crawler`

        **Returns**
        -------
        `middleware`
            instance of the middleware
        """
        
        ## Those patterns would not be parsed nor used for link follow logics
        s = cls(deny_patterns=crawler.settings.get('DENY_URL_PATTERNS_WIKIDOC', []))
        
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        
        """# üç™ **Status: _Default_**
        
        **Description**
        -----------
            This method is called when the request is made by the spider. It is used to process the request object.

        **Parameters**
        ----------
            `request`: `Request`
            `spider`: `Spider`

        **Returns**
        -------
        `None`
            a.k.a. Proceeding with the request.
        """
        return None

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.
        
        """# üç™ **Status: _Implemented_**
        
        **Description**
        -----------
            This method is called when the response is received by the spider. It is used to validate the response \
            and clean the HTML content of the response object and reconstruct the response object. It calls the clean_html() \
            method to clean the HTML content of the response object and embeds the cleaned HTML content in the meta field of \
            the response object.
            
        **Parameters**
        ----------
            `request`: `Request`
            `response`: `Response`
            `spider`: `Spider`

        **Returns**
        -------
        `response`
            cleaned response object to be processed by the spider

        Raises
        ------
        IgnoreRequest
            based on the following conditions:
            - 400 < - - - > 499 range code
            - 300 < - - - > 399 range code
            - 500 < - - - > 599 range code
            - any code below 300 but not 200
        """

        spider.logger.info(f"MdkengineeringDownloaderMiddleware: Scraping url: {response.url}")
        if response.status >= 400 and response < 500:  
            ## 400 < - - - > 499 range code
            spider.logger.info(f"\nMdkengineeringDownloaderMiddleware: Invalid Response, response code: {response.status} \n Most likely not found or not allowed")
            
            if response.status == 403:
                spider.logger.info(f'MdkengineeringDownloaderMiddleware: {response.status} code response received, this machine might be banned by the web server')
            
            raise IgnoreRequest(f"MdkengineeringDownloaderMiddleware: Skipping {response.status} coded response from {response.url}")
        elif response.status >= 300 and response < 400:
            ## 300 < - - - > 399 range code
            spider.logger.info(f'\nMdkengineeringDownloaderMiddleware: Invalid Response, response code: {response.status} \n Redirect response received')
            raise IgnoreRequest(f"MdkengineeringDownloaderMiddleware: Skipping {response.status} coded response from {response.url}")
        elif response.status >= 500:
            ## 500 < - - - > 599 range code
            spider.logger.info(f'\nMdkengineeringDownloaderMiddleware: Invalid Response, response code: {response.status} \n Server side of the website screwed up')
            raise IgnoreRequest(f"MdkengineeringDownloaderMiddleware: Skipping {response.status} coded response from {response.url}")
        elif response.status != 200:
            ## any code below 300 but not 200
            spider.logger.info(f'\nMdkengineeringDownloaderMiddleware: Invalid Response, response code: {response.status} \n Content Quality Problems')
            raise IgnoreRequest(f"MdkengineeringDownloaderMiddleware: Skipping {response.status} coded response from {response.url}")
        elif isinstance(response, HtmlResponse):

            try:
                
                spider.logger.info(f'MdkengineeringDownloaderMiddleware: Cleaning html & Reconstructing response')
                cleaned_html = self.clean_html(response, spider)
                
                #### Follow schema of spider return item
                ## [x] 0. HTML Content
                ## [x] 1. last-modified (date it was last modified) : date
                ## [x] 2. source (url) : string [x]
                ## [x] 3. (page) title : string [x]
                ## [x] 5. (custom) id (url-ending+last_modified date) : string
                ## [x] 6. keyword : []
                ## [x] 7. authors: []
                ## [x] 8. path: string
                cleaned_response = {
                    'title': cleaned_html['title'],
                    'content': cleaned_html['content'], ## List of HTML Tags
                    'last-modified': cleaned_html['last-modified'],
                    'source': cleaned_html['source'],
                    'id': cleaned_html['id'],
                    'keywords': cleaned_html['keywords'],
                    'authors': cleaned_html['authors'],
                    'site-path': cleaned_html['site-path'],
                    'categories': cleaned_html['categories']
                }
                
                ## Remake HTML response
                new_response = HtmlResponse(
                    url=response.url,
                    status=response.status,
                    headers=response.headers,
                    body=response.body,  # Keep the original body
                    encoding=response.encoding,
                    request=request,
                )
                
                # Replace the original response's meta with the cleaned HTML
                new_response.meta['cleaned_details'] = cleaned_response
                
                return new_response

            except Exception as e:
                spider.logger.error(f"MdkengineeringDownloaderMiddleware: Error processing response: {e}")
                # Optionally, re-raise the exception or return None to let the process continue
                raise  # This will propagate the exception

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.
        
        """# üç™ **Status: _Not Implemented_**
        
        **Description**
        ------------
            Called when a download handler or a process_request() (from other downloader middleware) raises an exception.
            
        **Parameters**
        ----------
            `request`: `Request`
            `exception`: `Exception`
            `spider`: `Spider`
        """

        spider.logger.error(f"MdkengineeringDownloaderMiddleware: Exception occurred: {exception}")

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        
        """# üç™ **Status: _Implemented_**
        
        **Description**
        ------------
            Called when the spider is opened. This method is used to perform any initializations \
            that the spider may need before the spider starts crawling.
            
        **Parameters**
        ----------
            `spider`: `Spider`
        """
        spider.logger.info("MdkengineeringDownloaderMiddleware: Spider opened: %s" % spider.name)

    def clean_html(self, response, spider):
        
        """# üç™ **Status: _Implemented_**
        
        **Description**
        ------------
            This method is used to clean the HTML content of the response object and produces metadata \
            from the HTML content. It extracts the title, last-modified date, keywords, URL suffix, authors, and categories \
            from the HTML content. It also filters out the HTML content to remove unwanted tags and comments.
            
        **Parameters**
        ----------
            `response`: `HtmlResponse`
            `spider`: `Spider`
            
        **Notes**
        -----
            -  [x] Meta Data Extractions 
                
            | **Field**            | ****Description****                                                                                             |
            |:---------------------|:-----------------------------------------------------------------------------------------------------------|
            | `title`              | h1 heading text of the wikidoc page                                                                         |
            | `last_modified_date` | - Date when the HTML in the response object was last edited <br> - In `DateTime._Date` object format        |
            | `keywords`           | - Keywords associated with the article in HTML <br> - Extracted from the HTML content                        |
            | `url_suffix`         | Portion of the URL after `index.php/`                                                                       |
            | `authors`            | - Authors who created and/or edited the medical content in HTML <br> - Extracted from HTML content            |
            | `categories`         | - Categories related to the textual content in HTML <br> - Extracted from the HTML content                   |
    
        Stages
        ------
            1. Primary Cleaning ~ Filter page structures or metadata:
                - Extract information under the <div> with class 'mw-parser-output'
                    **  <div class="mw-parser-output ..." ... />
            2. Secondary Cleaning ~ Filter complexly formatted information or programmatically assistive information:
                - Extract all from 'content' except:
                    1. tag with class infobox
                    2. comments
                    3. referencing sections and labels
                    4. remaining metadata under 'content'
            3. Tertiary Cleaning ~ Remove duplicates returned by 'xpath' method:
                - Nested tags would be extracted as a separated entry,
                    so this for loop is to remove nested subtags
                    
        **Returns**
        -------
        `dict`
            a dictionary containing the cleaned HTML content and metadata extracted from the HTML content.
        """
        
        returner = []
        
        title = response.xpath('//h1[contains(@id, "firstHeading")]/span[contains(@class, "mw-page-title-main")]/text()').extract()
        last_modified = response.xpath('//*[@id="footer-info-credits"]/text()').extract()[0].split(' by ')[0].split(', ')[-1]
        last_modified_date = datetime.strptime(last_modified, "%d %B %Y").date()
        keyword_extract = response.xpath('//p[descendant::i[contains(., "Synonyms and keywords")]]/text()')
        keywords = []
        if len(keyword_extract) > 0:
            keywords = keyword_extract.extract()[0].replace('Synonyms and keywords', '').replace('.\n', '').split('; ')
        url_suffix = str(response.url).split('index.php')[1]
        authors = response.xpath('//p[contains(., "Editor")]/a[contains(@title, "User:")]/text()').extract()
        categories = response.xpath('//*[@id="mw-normal-catlinks"]/ul/li/a[contains(@title, "Category:")]/text()').extract() 
        
        try:
            
            ## Primary Cleaning
            content = response.xpath('//div[contains(@class, "mw-parser-output")]')
             
            ## Secondary Cleaning
            filtered_content = content.xpath( 
                './/*[ \
                    not(ancestor-or-self::table[contains(@class, "infobox")]) \
                    and not(comment()) \
                    and not(contains(., "Template:")) \
                    and not(descendant-or-self::*[contains(@id, "References")]) \
                    and not(contains(@class, "references-column-width")) \
                    and not(ancestor::*[contains(@class, "references-column-width")]) \
                    and not(self::p[contains(., "Editor")]) \
                    and not(ancestor::p[contains(., "Editor")]) \
                    and not(ancestor::p[contains(., "Synonyms and keywords")]) \
                    and not(self::p[contains(., "Synonyms and keywords")]) \
                    and not(self::p[contains(., "For patient information")]) \
                    and not(ancestor::p[contains(., "For patient information")]) \
                    and not(self::sup[contains(@class, "reference")]) \
                    and not(ancestor-or-self::div[contains(@class, "toc")]) \
                    and not(self::div[contains(@role, "note")]) \
                ]'
            )
            
            ## Tertiary Cleaning
            for idx, item in enumerate(filtered_content.extract()):
                if idx == 0:
                    returner.append(item)
                    continue
                
                if item in returner[-1]:
                    continue
                else: 
                    returner.append(item)
            
            spider.logger.info(f'MdkengineeringDownloaderMiddleware: HTML Cleaning Completed.')
        except Exception as error:
            spider.logger.error(f'MdkengineeringDownloaderMiddleware: HTML cleaning Failed: {error}')
            raise error
        
        
        #### Follow Schema of Spider Return Item
        ## [x] 0. HTML Content
        ## [x] 1. last-modified (date it was last modified) : date
        ## [x] 2. source (url) : string 
        ## [x] 3. (page) title : string 
        ## [x] 5. (custom) id (url-ending+last_modified date) : string
        ## [x] 6. keyword : []
        ## [x] 7. authors: []
        ## [x] 8. site-path: string 
        ## [x] 9. categories: [string]
        if (len(returner) == 0):
            
            ## No error was thrown here due to children of 'mw-category-generated' class tag provides
            ## relevant links to medical knowledge for scraping
            
            spider.logger.info(f'MdkengineeringDownloaderMiddleware: Nothing was extracted')
            return {
                'title': title,
                'content': None, 
                'last-modified': last_modified_date,
                'source': response.url,
                'id': f'{url_suffix[1:].replace("_", "-").replace("/", "_")}__{str(last_modified_date)}',
                'keywords': keywords,
                'authors': authors,
                'site-path': url_suffix,
                'categories': categories
            }
        else:
            return {
                'title': title,
                'content': returner, ## List of HTML Tags
                'last-modified': last_modified_date,
                'source': response.url,
                'id': f'{url_suffix[1:].replace("_", "-").replace("/", "_")}__{str(last_modified_date)}',
                'keywords': keywords,
                'authors': authors,
                'site-path': url_suffix,
                'categories': categories
            }
